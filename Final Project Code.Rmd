---
output:
  word_document: default
  html_document: default
---
# Appendices

## R Code
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Load necessary libraries
library(readr)
library(dplyr)
library(ggplot2)
library(MASS)
library(caret)
library(randomForest)

# Load dataset
uber <- read_csv("uber.csv")

# Data cleaning
uber <- uber %>%
  filter(
    pickup_longitude >= -74.3 & pickup_longitude <= -73.7,
    pickup_latitude >= 40.5 & pickup_latitude <= 41.0,
    dropoff_longitude >= -74.3 & dropoff_longitude <= -73.7,
    dropoff_latitude >= 40.5 & dropoff_latitude <= 41.0,
    fare_amount > 0 & fare_amount < 100,
    passenger_count >= 1 & passenger_count <= 6
  ) %>%
  mutate(
    pickup_latitude_category = cut(pickup_latitude, breaks = 4, labels = c("South", "Central South", "Central North", "North")),
    pickup_longitude_category = cut(pickup_longitude, breaks = 4, labels = c("West", "Central West", "Central East", "East"))
  )

# Inspect dataset
summary(uber)
```

# Analysis

## Data Splitting

The dataset was split into training (70%) and testing (30%) sets to enable out-of-sample evaluation.

```{r data-splitting, echo=TRUE}
set.seed(123)
train_indices <- sample(seq_len(nrow(uber)), size = 0.7 * nrow(uber))
train_data <- uber[train_indices, ]
test_data <- uber[-train_indices, ]
```

## Stepwise Regression

Step-wise regression was performed for variable selection using AIC. Diagnostics plots were used to assess model assumptions.

```{r stepwise-regression, echo=TRUE}
# Full model for step-wise regression
full_model <- lm(fare_amount ~ pickup_latitude_category + pickup_longitude_category + 
                   dropoff_latitude + dropoff_longitude + passenger_count, data = train_data)

# Step-wise selection
stepwise_model <- stepAIC(full_model, direction = "both")
summary(stepwise_model)

# Diagnostics
par(mfrow = c(2, 2))
plot(stepwise_model)
```

## Log-Transformed Regression

Log transformation was applied to stabilize variance and normalize the dependent variable. This model aims to improve prediction accuracy for non-linear relationships.

```{r log-transformation, echo=TRUE}
# Apply log transformation
train_data <- train_data %>% mutate(log_fare_amount = log(fare_amount))
test_data <- test_data %>% mutate(log_fare_amount = log(fare_amount))

# Log-transformed regression
log_model <- lm(log_fare_amount ~ pickup_latitude_category + pickup_longitude_category + 
                  dropoff_latitude + dropoff_longitude + passenger_count, data = train_data)
summary(log_model)
```

## Polynomial Regression

Polynomial terms for latitude and longitude were added to capture potential non-linear relationships in the data.

```{r polynomial-regression, echo=TRUE}
poly_model <- lm(fare_amount ~ pickup_latitude_category + pickup_longitude_category + 
                   poly(dropoff_latitude, 2) + poly(dropoff_longitude, 2) + passenger_count, data = train_data)
summary(poly_model)
```

## Random Forest

Random forest, a non-parametric model, was implemented to capture complex interactions between predictors. A sub sample of the training data was used for computational efficiency.

```{r random-forest, echo=TRUE}
set.seed(123)
train_sample <- train_data %>% sample_n(5000)  # Use a smaller sample
rf_model <- randomForest(
  fare_amount ~ pickup_latitude_category + pickup_longitude_category + 
    dropoff_latitude + dropoff_longitude + passenger_count,
  data = train_sample,
  ntree = 50,
  mtry = 3,
  importance = TRUE
)

print(rf_model)
varImpPlot(rf_model)
```

# Model Evaluation

Model performance was evaluated using RMSE on the test data. 

```{r model-evaluation, echo=TRUE}
# Predictions and RMSE calculations
linear_predictions <- predict(stepwise_model, test_data)
linear_rmse <- sqrt(mean((linear_predictions - test_data$fare_amount)^2))

log_predictions <- predict(log_model, test_data)
log_rmse <- sqrt(mean((exp(log_predictions) - test_data$fare_amount)^2))

poly_predictions <- predict(poly_model, test_data)
poly_rmse <- sqrt(mean((poly_predictions - test_data$fare_amount)^2))

rf_predictions <- predict(rf_model, test_data)
rf_rmse <- sqrt(mean((rf_predictions - test_data$fare_amount)^2))

# Comparison table
model_comparison <- data.frame(
  Model = c("Stepwise Regression", "Log-Transformed Regression", "Polynomial Regression", "Random Forest"),
  RMSE = c(linear_rmse, log_rmse, poly_rmse, rf_rmse)
)

print(model_comparison)
```

# Visualization

```{r visualization, echo=TRUE}
# RMSE comparison plot
ggplot(model_comparison, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity") +
  labs(title = "Model RMSE Comparison", x = "Model", y = "RMSE") +
  theme_minimal()
```

# Results and Discussion

- **Step-wise Regression:** Captured linear relationships but showed limitations in capturing non-linear patterns.
- **Log-Transformed Regression:** Stabilized variance but under-performed due to residual non-linearities.
- **Polynomial Regression:** Improved flexibility with non-linear patterns but risked over-fitting.
- **Random Forest:** Outperformed all models by capturing complex interactions, with an RMSE of `r rf_rmse`.

# Conclusion

The random forest model is recommended for its superior predictive accuracy and ability to handle non-linearities. Future work could explore tuning hyper-parameters and incorporating additional predictors to further improve performance.
